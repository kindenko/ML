{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW6_DE",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RumFnRay9oEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63cfa432-97ff-4999-c495-ea5c16c2cca7"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 64kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 41.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=c69e5fcb3206577cebf43bcb428858a4fafad68ffde066bd9f162f993457e8a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u275-b01-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u275-b01-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u275-b01-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u275-b01-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u275-b01-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u275-b01-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68_AeFo49pEU"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftRUvQn89-yC"
      },
      "source": [
        "id='1f_9EbnywCj35EBUA32sueigxBjBJwALr'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('War and Peace by Leo Tolstoy (ru).txt')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpR8DQxm-IRN"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6qPAe12-Q19"
      },
      "source": [
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E-RdW9y-U1o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "71f67bd5-a195-4af8-868f-52d6eecfa531"
      },
      "source": [
        "spark"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7c3ca5caca0e:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f69c3569630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6LU3RGy-Yag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec44d885-e79e-488d-fd68-39225e6cceef"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-22 09:40:59--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.204.93.39, 34.195.187.253, 52.202.213.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.204.93.39|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  37.9MB/s    in 0.3s    \n",
            "\n",
            "2020-12-22 09:41:00 (37.9 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgte5waWKrpd"
      },
      "source": [
        "Подсчитай кол-во слов в документе \"War and Peace by Leo Tolstoy (ru).txt\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7AYXXsTKqjm"
      },
      "source": [
        "# код\r\n",
        "# перекодировка в utf_8\r\n",
        "f = open('War and Peace by Leo Tolstoy (ru).txt', 'r',encoding='cp1251')\r\n",
        "text = f.read().encode(\"utf-8\").decode(\"utf-8\")\r\n",
        "\r\n",
        "utf_8_text = open(\"utf_8_text.txt\", \"w\")\r\n",
        "utf_8_text.write(text)\r\n",
        "utf_8_text.close()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkUGMUV-MvOx"
      },
      "source": [
        "def lower_clean(x):\r\n",
        "  punc='!\"#$%\\'&*(),.;<=>?@[\\\\]^_`{|}~-'\r\n",
        "  lowered = x.lower()\r\n",
        "  for ch in punc:\r\n",
        "    lowered = lowered.replace(ch,'')\r\n",
        "  lowered = lowered.replace('\\t','')\r\n",
        "  return lowered"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obrtyNUMcZc3"
      },
      "source": [
        "book = sc.textFile('utf_8_text.txt').map(lower_clean) # читаем файл и чистим\r\n",
        "clean_book = book.flatMap(lambda line: line.split(\" \")) # разделяем на слова --> лист слов \r\n",
        "clean_book = clean_book.filter(lambda word: word != '') # удаляем пустые\r\n",
        "text = clean_book.map(lambda word: (word,1)) # мэпим для каждого слова единичку --> лист \"слово - единица\"\r\n",
        "reduced_text = text.reduceByKey(lambda x,y: x+y) # для всех одинаковых ключей(слов) складываем значения --> количество повторений каждого слова\r\n",
        "Sum = reduced_text.map(lambda word: word[1]).sum() # "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2SXqJe-DkMv",
        "outputId": "fc6bf0a4-05d6-4cbf-9021-8d8673ee4fe5"
      },
      "source": [
        "print(reduced_text.take(3))\r\n",
        "print(Sum)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('война', 71), ('и', 21470), ('мир', 50)]\n",
            "474044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJgmIgCzLJTo"
      },
      "source": [
        "id='13yfAoONwq4rS5XrTv3IrcqcFcdgfvK9V'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('mnist-digits-train.txt')\n",
        "\n",
        "id='1VE_9x0LQvOJpHXbXp_RMPl3Q4wRUuOok'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('mnist-digits-test.txt')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSU-TTUvNaON"
      },
      "source": [
        "Необходимо обучить модель используя Spark MLlib (модель на ваш выбор, например Decision Tree) и получить accuracy.\n",
        "Подробнее тут: https://spark.apache.org/docs/latest/ml-classification-regression.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saTc8AVOLiJq"
      },
      "source": [
        "# код\r\n",
        "\r\n",
        "train = spark.read.format(\"libsvm\").option(\"numFeatures\",\"780\").load('mnist-digits-train.txt')\r\n",
        "test = spark.read.format(\"libsvm\").option(\"numFeatures\",\"780\").load('mnist-digits-test.txt')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKYfC1o3LF2g",
        "outputId": "24486310-0331-47e9-973e-aaad1d1273fc"
      },
      "source": [
        "len(test.head()[1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "780"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2S2naFdkjQe"
      },
      "source": [
        "from pyspark.sql.functions import UserDefinedFunction\r\n",
        "from pyspark.sql.types import StringType"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxgnIXF55UQj"
      },
      "source": [
        "#******* not needed\r\n",
        "from pyspark.sql.functions import UserDefinedFunction\r\n",
        "import pyspark.ml.linalg\r\n",
        "def add_2_empty_col_(v):\r\n",
        "    v = DenseVector(v)\r\n",
        "    new_array = list([float(x) for x in v])\r\n",
        "    new_array.append(0)\r\n",
        "    new_array.append(0)\r\n",
        "    v = pyspark.ml.linalg.SparseVector(new_array)\r\n",
        "    return v\r\n",
        "name = 'features'\r\n",
        "udf = UserDefinedFunction(lambda x: add_2_empty_col_(x),T.ArrayType(T.FloatType()))\r\n",
        "test_test = test.select(*[udf(column).alias(name) if column == name else column for column in test.columns])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7UupD04cJVG",
        "outputId": "459f3714-e19d-40a6-afed-a8d9a608f55b"
      },
      "source": [
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\r\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\r\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\r\n",
        "labelIndexer = StringIndexer(inputCol='label',outputCol='indexedLabel').fit(train)\r\n",
        "featureIndexer = VectorIndexer(inputCol='features',outputCol='indexedFeatures',handleInvalid='skip').fit(train)\r\n",
        "# Train a DecisionTree model.\r\n",
        "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\r\n",
        "\r\n",
        "# Chain indexers and tree in a Pipeline\r\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\r\n",
        "\r\n",
        "# Train model.  This also runs the indexers.\r\n",
        "model = pipeline.fit(train)\r\n",
        "\r\n",
        "# Make predictions.\r\n",
        "predictions = model.transform(test)\r\n",
        "\r\n",
        "# Select example rows to display.\r\n",
        "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\r\n",
        "\r\n",
        "# Select (prediction, true label) and compute test error\r\n",
        "evaluator = MulticlassClassificationEvaluator(\r\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\r\n",
        "accuracy = evaluator.evaluate(predictions)\r\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))\r\n",
        "\r\n",
        "treeModel = model.stages[2]\r\n",
        "# summary only\r\n",
        "print(treeModel)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------------+--------------------+\n",
            "|prediction|indexedLabel|            features|\n",
            "+----------+------------+--------------------+\n",
            "|       1.0|         1.0|(780,[202,203,204...|\n",
            "|       6.0|         3.0|(780,[94,95,96,97...|\n",
            "|       0.0|         0.0|(780,[128,129,130...|\n",
            "|       5.0|         5.0|(780,[124,125,126...|\n",
            "|       4.0|         8.0|(780,[150,151,159...|\n",
            "+----------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Test Error = 0.321601 \n",
            "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_5ae3b83eefd6, depth=5, numNodes=59, numClasses=10, numFeatures=780\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKMztZ2eOOrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e89c40bf-953b-4b1d-977e-3722314d442a"
      },
      "source": [
        "id='1kUIrskM0zNH8u71G9M1BkHjRQYxvgAvh'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('data.zip')\n",
        "!unzip data.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/regions.csv        \n",
            "  inflating: data/departments.csv    \n",
            "  inflating: data/jobs.csv           \n",
            "  inflating: data/locations.csv      \n",
            "  inflating: data/country.csv        \n",
            "  inflating: data/employees.csv      \n",
            "  inflating: data/job_history.csv    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v43K2fDAjUYb"
      },
      "source": [
        "Кто получает больше всего? Кто меньше всего?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znR8_Kh_IXL4"
      },
      "source": [
        "pd.read_csv(\"data/jobs.csv\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90Xg1NAQEgQI"
      },
      "source": [
        "jobs_df = spark.read.option(\"delimiter\", \"\\t\").load(\"data/jobs.csv\", format=\"csv\")\r\n",
        "jobs_df = jobs_df.withColumnRenamed(\"_c0\", \"JOB_ID\")\r\n",
        "jobs_df = jobs_df.withColumnRenamed(\"_c1\", \"JOB_TITLE\")\r\n",
        "jobs_df = jobs_df.withColumnRenamed(\"_c2\", \"MIN_SALARY\")\r\n",
        "jobs_df = jobs_df.withColumnRenamed(\"_c3\", \"MAX_SALARY\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm15EW8znWDo"
      },
      "source": [
        "jobs_df.createOrReplaceTempView(\"JOBS_DF\")\r\n",
        "sqlDF = spark.sql(\"SELECT JOB_TITLE, ((MAX_SALARY+MIN_SALARY)/2) AS MEAN_SALARY FROM JOBS_DF ORDER BY MEAN_SALARY DESC\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daKwW1BfKaJ5",
        "outputId": "eeaa6de6-ea6c-48f6-b745-53d21568f1f4"
      },
      "source": [
        "sqlDF.collect()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(JOB_TITLE='President', MEAN_SALARY=30040.0),\n",
              " Row(JOB_TITLE='Administration Vice President', MEAN_SALARY=22500.0),\n",
              " Row(JOB_TITLE='Sales Manager', MEAN_SALARY=15040.0),\n",
              " Row(JOB_TITLE='Finance Manager', MEAN_SALARY=12100.0),\n",
              " Row(JOB_TITLE='Accounting Manager', MEAN_SALARY=12100.0),\n",
              " Row(JOB_TITLE='Marketing Manager', MEAN_SALARY=12000.0),\n",
              " Row(JOB_TITLE='Purchasing Manager', MEAN_SALARY=11500.0),\n",
              " Row(JOB_TITLE='Sales Representative', MEAN_SALARY=9004.0),\n",
              " Row(JOB_TITLE='Public Relations Representative', MEAN_SALARY=7500.0),\n",
              " Row(JOB_TITLE='Stock Manager', MEAN_SALARY=7000.0),\n",
              " Row(JOB_TITLE='Programmer', MEAN_SALARY=7000.0),\n",
              " Row(JOB_TITLE='Accountant', MEAN_SALARY=6600.0),\n",
              " Row(JOB_TITLE='Public Accountant', MEAN_SALARY=6600.0),\n",
              " Row(JOB_TITLE='Marketing Representative', MEAN_SALARY=6500.0),\n",
              " Row(JOB_TITLE='Human Resources Representative', MEAN_SALARY=6500.0),\n",
              " Row(JOB_TITLE='Administration Assistant', MEAN_SALARY=4500.0),\n",
              " Row(JOB_TITLE='Purchasing Clerk', MEAN_SALARY=4000.0),\n",
              " Row(JOB_TITLE='Shipping Clerk', MEAN_SALARY=4000.0),\n",
              " Row(JOB_TITLE='Stock Clerk', MEAN_SALARY=3504.0),\n",
              " Row(JOB_TITLE='JOB_TITLE', MEAN_SALARY=None)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg8-rkFlkMqm"
      },
      "source": [
        "Выведете топ 5 по зарплате."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMzJ_tJhnWuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e25520d-fcab-46d1-aef3-6c90217318f2"
      },
      "source": [
        "sqlDF = spark.sql(\"SELECT JOB_TITLE, ((MAX_SALARY+MIN_SALARY)/2) AS MEAN_SALARY FROM JOBS_DF ORDER BY MEAN_SALARY DESC LIMIT 5\")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(JOB_TITLE='President', MEAN_SALARY=30040.0),\n",
              " Row(JOB_TITLE='Administration Vice President', MEAN_SALARY=22500.0),\n",
              " Row(JOB_TITLE='Sales Manager', MEAN_SALARY=15040.0),\n",
              " Row(JOB_TITLE='Finance Manager', MEAN_SALARY=12100.0),\n",
              " Row(JOB_TITLE='Accounting Manager', MEAN_SALARY=12100.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfBpDVYwkM7K"
      },
      "source": [
        "Подгружаем оставшиеся таблицы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsHT2InyQjr9"
      },
      "source": [
        "pd.read_csv(\"data/regions.csv\", sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOcZLFtiR0_y"
      },
      "source": [
        "pd.read_csv(\"data/country.csv\", sep='\\t').head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j36Sd5qbRrwB"
      },
      "source": [
        "pd.read_csv(\"data/locations.csv\", sep='\\t').head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLuIgdwIRSys"
      },
      "source": [
        "pd.read_csv(\"data/departments.csv\", sep='\\t').head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2d2AiBknYSo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "2e7e013d-fe01-4e7f-a31c-84fdd7d04d1b"
      },
      "source": [
        "pd.read_csv(\"data/employees.csv\", sep='\\t', comment='#').head()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EMPLOYEE_ID</th>\n",
              "      <th>FIRST_NAME</th>\n",
              "      <th>LAST_NAME</th>\n",
              "      <th>EMAIL</th>\n",
              "      <th>PHONE_NUMBER</th>\n",
              "      <th>HIRE_DATE</th>\n",
              "      <th>JOB_ID</th>\n",
              "      <th>SALARY</th>\n",
              "      <th>COMMISSION_PCT</th>\n",
              "      <th>MANAGER_ID</th>\n",
              "      <th>DEPARTMENT_ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100</td>\n",
              "      <td>Steven</td>\n",
              "      <td>King</td>\n",
              "      <td>SKING</td>\n",
              "      <td>515.123.4567</td>\n",
              "      <td>17.06.03</td>\n",
              "      <td>AD_PRES</td>\n",
              "      <td>24000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>101</td>\n",
              "      <td>Neena</td>\n",
              "      <td>Kochhar</td>\n",
              "      <td>NKOCHHAR</td>\n",
              "      <td>515.123.4568</td>\n",
              "      <td>21.09.05</td>\n",
              "      <td>AD_VP</td>\n",
              "      <td>17000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>102</td>\n",
              "      <td>Lex</td>\n",
              "      <td>De Haan</td>\n",
              "      <td>LDEHAAN</td>\n",
              "      <td>515.123.4569</td>\n",
              "      <td>13.01.01</td>\n",
              "      <td>AD_VP</td>\n",
              "      <td>17000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>103</td>\n",
              "      <td>Alexander</td>\n",
              "      <td>Hunold</td>\n",
              "      <td>AHUNOLD</td>\n",
              "      <td>590.423.4567</td>\n",
              "      <td>03.01.06</td>\n",
              "      <td>IT_PROG</td>\n",
              "      <td>9000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>102.0</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>104</td>\n",
              "      <td>Bruce</td>\n",
              "      <td>Ernst</td>\n",
              "      <td>BERNST</td>\n",
              "      <td>590.423.4568</td>\n",
              "      <td>21.05.07</td>\n",
              "      <td>IT_PROG</td>\n",
              "      <td>6000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>103.0</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   EMPLOYEE_ID FIRST_NAME LAST_NAME  ... COMMISSION_PCT MANAGER_ID DEPARTMENT_ID\n",
              "0          100     Steven      King  ...            NaN        NaN            90\n",
              "1          101      Neena   Kochhar  ...            NaN      100.0            90\n",
              "2          102        Lex   De Haan  ...            NaN      100.0            90\n",
              "3          103  Alexander    Hunold  ...            NaN      102.0            60\n",
              "4          104      Bruce     Ernst  ...            NaN      103.0            60\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSp8LT-_Ti1n"
      },
      "source": [
        "regions_df = spark.read.option(\"delimiter\", \"\\t\").load(\"data/regions.csv\", format=\"csv\").toDF(\r\n",
        "  \"REGION_ID\", \"REGION_NAME\"\r\n",
        ")\r\n",
        "regions_df.createOrReplaceTempView(\"regions_df\")\r\n",
        "country_df = spark.read.option(\"delimiter\", \"\\t\").load(\"data/country.csv\", format=\"csv\").toDF(\r\n",
        "  \"COUNTRY_ID\", \"COUNTRY_NAME\", \"REGION_ID\"\r\n",
        ")\r\n",
        "country_df.createOrReplaceTempView(\"country_df\")\r\n",
        "locations_df = spark.read.option(\"delimiter\", \"\\t\").load(\"data/locations.csv\", format=\"csv\").toDF(\r\n",
        "  \"LOCATION_ID\", \"STREET_ADDRESS\", \"POSTAL_CODE\", \"CITY\", \"STATE_PROVINCE\", \"COUNTRY_ID\"\r\n",
        ")\r\n",
        "locations_df.createOrReplaceTempView(\"locations_df\")\r\n",
        "departments_df = spark.read.option(\"delimiter\", \"\\t\").load(\"data/departments.csv\", format=\"csv\").toDF(\r\n",
        "  \"DEPARTMENT_ID\", \"DEPARTMENT_NAME\", \"MANAGER_ID\", \"LOCATION_ID\"\r\n",
        ")\r\n",
        "departments_df.createOrReplaceTempView(\"departments_df\")\r\n",
        "employees_df = spark.read.option(\"delimiter\", \"\\t\").load(\"data/employees.csv\", format=\"csv\").toDF(\r\n",
        "  \"EMPLOYEE_ID\", \"FIRST_NAME\", \"LAST_NAME\", \"EMAIL\", \"PHONE_NUMBER\", \"HIRE_DATE\", \"JOB_ID\", \"SALARY\", \"COMMISSION_PCT\", \"MANAGER_ID\", \"DEPARTMENT_ID\"\r\n",
        ")\r\n",
        "employees_df.createOrReplaceTempView(\"employees_df\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH-MAaxNXvQZ"
      },
      "source": [
        "Сколько всего регионов? Сколько работников в каждом регионе?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma3OqJHYR5R4"
      },
      "source": [
        "Связь regions - country - location - department - employer\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq8KyTRRahpQ",
        "outputId": "1169ae71-c21a-45d0-cbec-18cf5f406d87"
      },
      "source": [
        "sqlDF = spark.sql(\"\\\r\n",
        "  SELECT DISTINCT REGION_ID\\\r\n",
        "  FROM regions_df\")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(REGION_ID='3'),\n",
              " Row(REGION_ID='REGION_ID'),\n",
              " Row(REGION_ID='1'),\n",
              " Row(REGION_ID='4'),\n",
              " Row(REGION_ID='2')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcS378NpS6WN",
        "outputId": "c56858d6-7735-4956-b3f0-a153cbb04d1a"
      },
      "source": [
        "sqlDF = spark.sql(\"\\\r\n",
        "  SELECT  r.REGION_ID,COUNT(EMPLOYEE_ID) \\\r\n",
        "  FROM employees_df as e \\\r\n",
        "  JOIN departments_df as d on e.DEPARTMENT_ID = d.DEPARTMENT_ID\\\r\n",
        "  JOIN locations_df as l on d.LOCATION_ID = l.LOCATION_ID\\\r\n",
        "  JOIN country_df as c on l.COUNTRY_ID = c.COUNTRY_ID\\\r\n",
        "  JOIN regions_df as r on c.REGION_ID = r.REGION_ID\\\r\n",
        "  GROUP BY r.REGION_ID\")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(REGION_ID='REGION_ID', count(EMPLOYEE_ID)=1),\n",
              " Row(REGION_ID='1', count(EMPLOYEE_ID)=36),\n",
              " Row(REGION_ID='2', count(EMPLOYEE_ID)=70)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOY2unJ8kNXz"
      },
      "source": [
        "Выведете всех работников из Китая."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Lhj9GAwnZgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c70c8f19-d84f-4956-f3ae-95173b485511"
      },
      "source": [
        "sqlDF = spark.sql(\"\\\r\n",
        "  SELECT  e.EMPLOYEE_ID,c.COUNTRY_NAME  \\\r\n",
        "  FROM employees_df as e \\\r\n",
        "  JOIN departments_df as d on e.DEPARTMENT_ID = d.DEPARTMENT_ID\\\r\n",
        "  JOIN locations_df as l on d.LOCATION_ID = l.LOCATION_ID\\\r\n",
        "  JOIN country_df as c on l.COUNTRY_ID = c.COUNTRY_ID\\\r\n",
        "  WHERE c.COUNTRY_NAME = 'China'\")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Nc8F-6kNR7"
      },
      "source": [
        "Укажите самую высокооплачиваемою должность."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZNAohM-naSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab3fcc9-3bd7-4bbc-ce31-62ed3e506636"
      },
      "source": [
        "sqlDF = spark.sql(\"SELECT JOB_TITLE, ((MAX_SALARY+MIN_SALARY)/2) AS MEAN_SALARY FROM JOBS_DF ORDER BY MEAN_SALARY DESC LIMIT 1\")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(JOB_TITLE='President', MEAN_SALARY=30040.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzenJwUnkNL8"
      },
      "source": [
        "Выведете всех работников связанных с ИТ. Выведете их менеджеров. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eogYiLjXna3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e61d4de9-74f8-4322-cfc3-eea0e15f830b"
      },
      "source": [
        "sqlDF = spark.sql(\"\\\r\n",
        "  SELECT  EMPLOYEE_ID,MANAGER_ID   \\\r\n",
        "  FROM employees_df as e \\\r\n",
        "  WHERE JOB_ID = 'IT_PROG'\")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(EMPLOYEE_ID='103', MANAGER_ID='102'),\n",
              " Row(EMPLOYEE_ID='104', MANAGER_ID='103'),\n",
              " Row(EMPLOYEE_ID='105', MANAGER_ID='103'),\n",
              " Row(EMPLOYEE_ID='106', MANAGER_ID='103'),\n",
              " Row(EMPLOYEE_ID='107', MANAGER_ID='103')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvpCoeYPmLTW"
      },
      "source": [
        "Выведете имя и фамилию работника, его текущую и предыдущую должности и сколько полных недель и дней прошло с момент изменения. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57CGSSi5in8r"
      },
      "source": [
        "job_history_df = spark.read.option(\"delimiter\", \"\\t\").load(\"data/job_history.csv\", format=\"csv\").toDF(\r\n",
        "  \"EMPLOYEE_ID\", \"START_DATE\", \"END_DATE\", \"JOB_ID\", \"DEPARTMENT_ID\"\r\n",
        ")\r\n",
        "job_history_df=job_history_df.where(\"START_DATE!='START_DATE'\")# дропаем лишнюю строку\r\n"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkCvyzkVnbo6"
      },
      "source": [
        "pd.read_csv(\"data/job_history.csv\", sep='\\t', comment='#')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEw7pK5urTHn"
      },
      "source": [
        "# Необходимо исправить дату для того, чтобы пользоваться функциями sql\r\n",
        "from datetime import datetime\r\n",
        "import pyspark.sql.types as T\r\n",
        "import pyspark.sql.functions as F\r\n",
        "\r\n",
        "def user_defined_timestamp(date_col):\r\n",
        "    _date = str(date_col).strip().split('.')\r\n",
        "    l = len(_date)\r\n",
        "    temp = _date[0]\r\n",
        "    if(int(_date[l-1])<50):\r\n",
        "      _date[0] = '20'+_date[l-1]\r\n",
        "    else:\r\n",
        "      _date[0] = '19'+_date[l-1]\r\n",
        "    _date[l-1]=temp\r\n",
        "    return _date[0]+'-'+_date[l-2]+'-'+_date[l-1]\r\n",
        "\r\n",
        "user_defined_timestamp_udf = F.udf(user_defined_timestamp, T.StringType())"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "114i4CJYqe0j"
      },
      "source": [
        "from pyspark.sql.functions import date_format\r\n",
        "#df.select(date_format('dt', 'ddMMyy').alias('date')).collect()\r\n",
        "job_history_df = job_history_df.withColumn('NEW_START_DATE', user_defined_timestamp_udf('START_DATE'))\r\n",
        "job_history_df = job_history_df.withColumn('NEW_END_DATE', user_defined_timestamp_udf('END_DATE'))\r\n",
        "job_history_df.createOrReplaceTempView(\"job_history_df\")"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffNrcFIXqqtl",
        "outputId": "1fcec102-b459-4e2f-abe8-e1aadc4cd714"
      },
      "source": [
        "job_history_df.collect()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(EMPLOYEE_ID='102', START_DATE='13.01.01', END_DATE='24.07.06', JOB_ID='IT_PROG', DEPARTMENT_ID='60', NEW_START_DATE='2001.01.13', NEW_END_DATE='2006.07.24'),\n",
              " Row(EMPLOYEE_ID='101', START_DATE='21.09.97', END_DATE='27.10.01', JOB_ID='AC_ACCOUNT', DEPARTMENT_ID='110', NEW_START_DATE='1997.09.21', NEW_END_DATE='2001.10.27'),\n",
              " Row(EMPLOYEE_ID='101', START_DATE='28.10.01', END_DATE='15.03.05', JOB_ID='AC_MGR', DEPARTMENT_ID='110', NEW_START_DATE='2001.10.28', NEW_END_DATE='2005.03.15'),\n",
              " Row(EMPLOYEE_ID='201', START_DATE='17.02.04', END_DATE='19.12.07', JOB_ID='MK_REP', DEPARTMENT_ID='20', NEW_START_DATE='2004.02.17', NEW_END_DATE='2007.12.19'),\n",
              " Row(EMPLOYEE_ID='114', START_DATE='24.03.06', END_DATE='31.12.07', JOB_ID='ST_CLERK', DEPARTMENT_ID='50', NEW_START_DATE='2006.03.24', NEW_END_DATE='2007.12.31'),\n",
              " Row(EMPLOYEE_ID='122', START_DATE='01.01.07', END_DATE='31.12.07', JOB_ID='ST_CLERK', DEPARTMENT_ID='50', NEW_START_DATE='2007.01.01', NEW_END_DATE='2007.12.31'),\n",
              " Row(EMPLOYEE_ID='200', START_DATE='17.09.95', END_DATE='17.06.01', JOB_ID='AD_ASST', DEPARTMENT_ID='90', NEW_START_DATE='1995.09.17', NEW_END_DATE='2001.06.17'),\n",
              " Row(EMPLOYEE_ID='176', START_DATE='24.03.06', END_DATE='31.12.06', JOB_ID='SA_REP', DEPARTMENT_ID='80', NEW_START_DATE='2006.03.24', NEW_END_DATE='2006.12.31'),\n",
              " Row(EMPLOYEE_ID='176', START_DATE='01.01.07', END_DATE='31.12.07', JOB_ID='SA_MAN', DEPARTMENT_ID='80', NEW_START_DATE='2007.01.01', NEW_END_DATE='2007.12.31'),\n",
              " Row(EMPLOYEE_ID='200', START_DATE='01.07.02', END_DATE='31.12.06', JOB_ID='AC_ACCOUNT', DEPARTMENT_ID='90', NEW_START_DATE='2002.07.01', NEW_END_DATE='2006.12.31')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjN4mPWuho3c",
        "outputId": "08426dd1-731f-4bd9-a624-23a521e11aff"
      },
      "source": [
        "sqlDF = spark.sql(\"\\\r\n",
        "  SELECT EMPLOYEE_ID, CAST(NEW_START_DATE AS date) \\\r\n",
        "  FROM job_history_df \\\r\n",
        "  WHERE EMPLOYEE_ID = 101\")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(EMPLOYEE_ID='101', NEW_START_DATE=datetime.date(1997, 9, 21)),\n",
              " Row(EMPLOYEE_ID='101', NEW_START_DATE=datetime.date(2001, 10, 28))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpJB0bBdm21c",
        "outputId": "eb369fa2-3977-402f-d507-92729f7c3464"
      },
      "source": [
        "sqlDF = spark.sql(\"\\\r\n",
        "  SELECT EMPLOYEE_ID, MAX(NEW_START_DATE) AS START_OF_CUR \\\r\n",
        "  FROM job_history_df \\\r\n",
        "  GROUP BY EMPLOYEE_ID \\\r\n",
        "  \")\r\n",
        "sqlDF.createOrReplaceTempView(\"CUR\")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(EMPLOYEE_ID='200', START_OF_CUR='2002-07-01'),\n",
              " Row(EMPLOYEE_ID='101', START_OF_CUR='2001-10-28'),\n",
              " Row(EMPLOYEE_ID='201', START_OF_CUR='2004-02-17'),\n",
              " Row(EMPLOYEE_ID='102', START_OF_CUR='2001-01-13'),\n",
              " Row(EMPLOYEE_ID='176', START_OF_CUR='2007-01-01'),\n",
              " Row(EMPLOYEE_ID='122', START_OF_CUR='2007-01-01'),\n",
              " Row(EMPLOYEE_ID='114', START_OF_CUR='2006-03-24')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQtRNPBZf2Nb",
        "outputId": "b4a4bf6a-3025-4283-a090-71207406f4d3"
      },
      "source": [
        "sqlDF = spark.sql(\"\\\r\n",
        "  SELECT J.EMPLOYEE_ID, START_OF_CUR, J.JOB_ID  \\\r\n",
        "  FROM job_history_df AS J\\\r\n",
        "  JOIN  CUR AS C ON C.START_OF_CUR = J.NEW_START_DATE AND C.EMPLOYEE_ID = J.EMPLOYEE_ID\\\r\n",
        "  \")\r\n",
        "sqlDF.createOrReplaceTempView(\"CUR\")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(EMPLOYEE_ID='102', START_OF_CUR='2001-01-13', JOB_ID='IT_PROG'),\n",
              " Row(EMPLOYEE_ID='101', START_OF_CUR='2001-10-28', JOB_ID='AC_MGR'),\n",
              " Row(EMPLOYEE_ID='201', START_OF_CUR='2004-02-17', JOB_ID='MK_REP'),\n",
              " Row(EMPLOYEE_ID='114', START_OF_CUR='2006-03-24', JOB_ID='ST_CLERK'),\n",
              " Row(EMPLOYEE_ID='122', START_OF_CUR='2007-01-01', JOB_ID='ST_CLERK'),\n",
              " Row(EMPLOYEE_ID='176', START_OF_CUR='2007-01-01', JOB_ID='SA_MAN'),\n",
              " Row(EMPLOYEE_ID='200', START_OF_CUR='2002-07-01', JOB_ID='AC_ACCOUNT')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDmTmNDt8Zph",
        "outputId": "a26557db-6489-4afa-82ce-ab847ac397cb"
      },
      "source": [
        "#Работники с несколькими должностями И ИХ ТЕК РАБОТА\r\n",
        "sqlDF = spark.sql(\"\\\r\n",
        "  SELECT CUR.EMPLOYEE_ID,CUR.START_OF_CUR FROM (SELECT EMPLOYEE_ID, count(NEW_START_DATE) AS CNT \\\r\n",
        "  FROM job_history_df \\\r\n",
        "  GROUP BY EMPLOYEE_ID) \\\r\n",
        "  JOIN CUR ON CUR.EMPLOYEE_ID = __auto_generated_subquery_name.EMPLOYEE_ID\\\r\n",
        "  where CNT>1\\\r\n",
        "  \")\r\n",
        "sqlDF.createOrReplaceTempView(\"TEMP\")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(EMPLOYEE_ID='200', START_OF_CUR='2002-07-01'),\n",
              " Row(EMPLOYEE_ID='101', START_OF_CUR='2001-10-28'),\n",
              " Row(EMPLOYEE_ID='176', START_OF_CUR='2007-01-01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlxIBLxt-v73",
        "outputId": "7b49ce9e-ecef-46c6-d97a-c7beeb7aa77a"
      },
      "source": [
        "# УДАЛИЛИ ТЕК РАБОТУ ТЕХ, У КОГО БЫЛО МНОГО ДОЛЖНОСТЕЙ\r\n",
        "sqlDF = spark.sql(\"\\\r\n",
        "  SELECT EMPLOYEE_ID,NEW_START_DATE,NEW_END_DATE,JOB_ID \\\r\n",
        "  FROM job_history_df \\\r\n",
        "  WHERE (EMPLOYEE_ID,NEW_START_DATE) NOT IN (SELECT EMPLOYEE_ID,START_OF_CUR FROM TEMP)\\\r\n",
        "  \")\r\n",
        "sqlDF.createOrReplaceTempView(\"PREV\")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(EMPLOYEE_ID='102', NEW_START_DATE='2001-01-13', NEW_END_DATE='2006-07-24', JOB_ID='IT_PROG'),\n",
              " Row(EMPLOYEE_ID='101', NEW_START_DATE='1997-09-21', NEW_END_DATE='2001-10-27', JOB_ID='AC_ACCOUNT'),\n",
              " Row(EMPLOYEE_ID='201', NEW_START_DATE='2004-02-17', NEW_END_DATE='2007-12-19', JOB_ID='MK_REP'),\n",
              " Row(EMPLOYEE_ID='114', NEW_START_DATE='2006-03-24', NEW_END_DATE='2007-12-31', JOB_ID='ST_CLERK'),\n",
              " Row(EMPLOYEE_ID='122', NEW_START_DATE='2007-01-01', NEW_END_DATE='2007-12-31', JOB_ID='ST_CLERK'),\n",
              " Row(EMPLOYEE_ID='200', NEW_START_DATE='1995-09-17', NEW_END_DATE='2001-06-17', JOB_ID='AD_ASST'),\n",
              " Row(EMPLOYEE_ID='176', NEW_START_DATE='2006-03-24', NEW_END_DATE='2006-12-31', JOB_ID='SA_REP')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL49Qv0ZCLiJ",
        "outputId": "ca103110-e8dd-44b0-e069-3d2feeb9aeaf"
      },
      "source": [
        "sqlDF = spark.sql(\"\\\r\n",
        "  SELECT * FROM (SELECT EMPLOYEE_ID,MAX(NEW_END_DATE) AS END_OF_PREV_DATE  \\\r\n",
        "  FROM PREV \\\r\n",
        "  GROUP BY EMPLOYEE_ID) \\\r\n",
        "  WHERE EMPLOYEE_ID IN (SELECT EMPLOYEE_ID FROM TEMP)\\\r\n",
        "  \")\r\n",
        "sqlDF.createOrReplaceTempView(\"PREV_JOB\")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(EMPLOYEE_ID='200', END_OF_PREV_DATE='2001-06-17'),\n",
              " Row(EMPLOYEE_ID='101', END_OF_PREV_DATE='2001-10-27'),\n",
              " Row(EMPLOYEE_ID='176', END_OF_PREV_DATE='2006-12-31')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70-BvOqi-Ady",
        "outputId": "11f66bb4-66b4-4de3-c5eb-93bc78856ddb"
      },
      "source": [
        "# ПОЛУЧИЛИ ПРЕДЫДУЩИЕ ДОЛЖНОСТИ И ДАТЫ УВОЛЬНЕНИЯ\r\n",
        "sqlDF = spark.sql(\"\\\r\n",
        "  SELECT P.EMPLOYEE_ID, END_OF_PREV_DATE, J.JOB_ID  \\\r\n",
        "  FROM PREV_JOB AS P\\\r\n",
        "  JOIN  job_history_df AS J ON P.END_OF_PREV_DATE = J.NEW_END_DATE AND P.EMPLOYEE_ID = J.EMPLOYEE_ID\\\r\n",
        "  \")\r\n",
        "sqlDF.createOrReplaceTempView(\"PREV_JOB\")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(EMPLOYEE_ID='101', END_OF_PREV_DATE='2001-10-27', JOB_ID='AC_ACCOUNT'),\n",
              " Row(EMPLOYEE_ID='200', END_OF_PREV_DATE='2001-06-17', JOB_ID='AD_ASST'),\n",
              " Row(EMPLOYEE_ID='176', END_OF_PREV_DATE='2006-12-31', JOB_ID='SA_REP')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u4jqzrvEYaD",
        "outputId": "ac4c74be-8451-4b13-b0a5-f427f246a196"
      },
      "source": [
        "#РЕЗУЛЬТАТ\r\n",
        "sqlDF = spark.sql(\"\\\r\n",
        "  SELECT C.EMPLOYEE_ID , P.JOB_ID AS PREV_JOB, C.JOB_ID AS CUR_JOB, DATEDIFF(CAST(START_OF_CUR AS DATE),CAST(END_OF_PREV_DATE AS DATE)) AS DATE_DIFF,CAST(DATEDIFF(CAST(START_OF_CUR AS DATE),CAST(END_OF_PREV_DATE AS DATE))/7 AS INT) AS WEEK_DIFF \\\r\n",
        "  FROM PREV_JOB AS P\\\r\n",
        "  LEFT JOIN CUR AS C ON C.EMPLOYEE_ID = P.EMPLOYEE_ID\\\r\n",
        "  \")\r\n",
        "sqlDF.createOrReplaceTempView(\"FINAL\")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(EMPLOYEE_ID='101', PREV_JOB='AC_ACCOUNT', CUR_JOB='AC_MGR', DATE_DIFF=1, WEEK_DIFF=0),\n",
              " Row(EMPLOYEE_ID='200', PREV_JOB='AD_ASST', CUR_JOB='AC_ACCOUNT', DATE_DIFF=379, WEEK_DIFF=54),\n",
              " Row(EMPLOYEE_ID='176', PREV_JOB='SA_REP', CUR_JOB='SA_MAN', DATE_DIFF=1, WEEK_DIFF=0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOBhGAaanC0e"
      },
      "source": [
        "Выведете уникальные телефонные номера"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05BrXyrsncKf"
      },
      "source": [
        "sqlDF = spark.sql(\"\\\r\n",
        "  SELECT DISTINCT PHONE_NUMBER   \\\r\n",
        "  FROM employees_df as e \")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdf1VBv3nMR-"
      },
      "source": [
        "Есть ли сотрудники с одинаковыми фамилиями и сколько их."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apTLX1o6jy6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "335c0f08-96d3-49ef-f7b4-1587e42bedf4"
      },
      "source": [
        "sqlDF = spark.sql(\"\\\r\n",
        "  SELECT * FROM (SELECT LAST_NAME, COUNT(EMPLOYEE_ID) AS CNT \\\r\n",
        "  FROM employees_df as e \\\r\n",
        "  GROUP BY LAST_NAME) WHERE CNT > 1 \\\r\n",
        "  \")\r\n",
        "sqlDF.collect()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(LAST_NAME='Smith', CNT=2),\n",
              " Row(LAST_NAME='King', CNT=2),\n",
              " Row(LAST_NAME='Cambrault', CNT=2),\n",
              " Row(LAST_NAME='Taylor', CNT=2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    }
  ]
}